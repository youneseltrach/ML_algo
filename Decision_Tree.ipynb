{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOFw0IqcywjXZAk0tE2tAOo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youneseltrach/ML_algo/blob/main/Decision_Tree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this lab, I'll Implement a decision tree from scratch and use it to classify mushrooms as edible or poisonous.\n",
        "\n",
        "> The process of creating a decision tree involves these steps:\n",
        "1. Begin with all samples at the root node.\n",
        "2. Evaluate information gain for all possible feature splits, choose the highest.\n",
        "3. Divide the data set based on the chosen feature, generating left and right branches.\n",
        "4. Repeat splitting until the stopping criteria is met.\""
      ],
      "metadata": {
        "id": "ebSvVhzjfBJj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B9q-azXYe0uB"
      },
      "outputs": [],
      "source": [
        "# Import all the packages that we will need during this assignment\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Calculate the Entropy\n",
        "Write a helper function, `compute_entropy`, to calculate the entropy (impurity measure) at a node.\\\n",
        "The entropy is then calculated as\n",
        "\n",
        "$$H(p) = -p \\text{log}_2(p) - (1- p) \\text{log}_2(1- p)$$\n",
        "\n",
        "Where : ùëù is the fraction/probabilty of examples that have value = 1 in y"
      ],
      "metadata": {
        "id": "MB8ahHNFjloE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_entropy(y):  \n",
        "\n",
        "    entropy = 0.\n",
        "    if len(y) != 0 :\n",
        "\n",
        "        #calulate the probability of examples that have 1\n",
        "        p = len(y[y == 1]) / len(y)\n",
        "        \n",
        "        if p != 0 and p != 1:\n",
        "            entropy = -p*np.log2(p)-(1-p)*np.log2(1-p)\n",
        "        else:\n",
        "            entropy = 0.\n",
        "\n",
        "    return entropy"
      ],
      "metadata": {
        "id": "y16KRagaiMr2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WTXcTInvnAf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1)  Split the dataset"
      ],
      "metadata": {
        "id": "t34Bdzu7nv1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_dataset(X, node_indices, feature):\n",
        "  \n",
        "    left_indices = []\n",
        "    right_indices = []\n",
        "\n",
        "    for i in node_indices:   \n",
        "       if X[i][feature] == 1: \n",
        "           left_indices.append(i)\n",
        "       else:\n",
        "           right_indices.append(i)\n",
        "\n",
        "    return left_indices, right_indices"
      ],
      "metadata": {
        "id": "ynAIcOV7oDjX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2ajGlXwZoiw9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}